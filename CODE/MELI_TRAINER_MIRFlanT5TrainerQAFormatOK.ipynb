{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "urTTgkbu-70q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip uninstall -y fsspec gcsfs\n",
        "!pip install -q fsspec[http]==2024.6.1 gcsfs==2024.6.1\n",
        "!pip install -q datasets transformers torch accelerate bitsandbytes\n",
        "\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
        "from datasets import Dataset\n",
        "from typing import List, Dict, Tuple\n",
        "from google.colab import drive\n",
        "\n",
        "class MIRFlanT5TrainerQAFormat:\n",
        "    def __init__(self,\n",
        "                 base_model: str = \"google/flan-t5-base\",\n",
        "                 output_dir: str = \"mir_flan_t5_qa\"):\n",
        "        \"\"\"Initialize the FLAN-T5 trainer for MIR questions\"\"\"\n",
        "        print(\"Initializing MIR FLAN-T5 Trainer...\")\n",
        "        self.base_model = base_model\n",
        "        self.output_dir = output_dir\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        print(f\"Loading base model: {base_model}\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
        "        self.model = AutoModelForSeq2SeqLM.from_pretrained(base_model)\n",
        "\n",
        "        # Move model to GPU if available\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        print(f\"Using device: {self.device}\")\n",
        "        self.model = self.model.to(self.device)\n",
        "\n",
        "    def format_question(self, item: Dict) -> Dict:\n",
        "        \"\"\"Format a question for QA training\"\"\"\n",
        "        try:\n",
        "            # Get question text from input field (removing \"Medical Question: \" prefix)\n",
        "            question_text = item['input'].replace(\"Medical Question: \", \"\").strip()\n",
        "            correct_answer = item['output']\n",
        "\n",
        "            input_text = f\"\"\"Responde esta pregunta de la manera mÃ¡s concisa posible.\n",
        "\n",
        "Pregunta: {question_text}\"\"\"\n",
        "\n",
        "            return {\n",
        "                'input': input_text,\n",
        "                'output': correct_answer\n",
        "            }\n",
        "        except Exception as e:\n",
        "            print(f\"Error formatting question: {str(e)}\")\n",
        "            print(f\"Item structure: {json.dumps(item, indent=2)}\")\n",
        "            return None\n",
        "\n",
        "    def prepare_training_data(self, questions: List[Dict]) -> Tuple[Dataset, Dataset]:\n",
        "        \"\"\"Prepare and split training data\"\"\"\n",
        "        print(\"Preparing training data...\")\n",
        "        training_data = []\n",
        "\n",
        "        for i, question in enumerate(questions):\n",
        "            try:\n",
        "                formatted = self.format_question(question)\n",
        "                if formatted:\n",
        "                    training_data.append(formatted)\n",
        "\n",
        "                if (i + 1) % 100 == 0:\n",
        "                    print(f\"Processed {i + 1} questions...\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing question {i}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        print(f\"Created {len(training_data)} training examples\")\n",
        "\n",
        "        if len(training_data) == 0:\n",
        "            raise ValueError(\"No valid training examples were created!\")\n",
        "\n",
        "        # Convert to DataFrame\n",
        "        df = pd.DataFrame(training_data)\n",
        "\n",
        "        # Tokenize inputs and outputs\n",
        "        inputs = list(df[\"input\"])\n",
        "        outputs = list(df[\"output\"])\n",
        "\n",
        "        # Tokenize with padding and truncation\n",
        "        tokenized_inputs = self.tokenizer(\n",
        "            inputs, padding=True, truncation=True, max_length=512, return_tensors=\"pt\"\n",
        "        )\n",
        "        tokenized_outputs = self.tokenizer(\n",
        "            outputs, padding=True, truncation=True, max_length=128, return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        # Create tokenized dataset\n",
        "        tokenized_data = {\n",
        "            \"input_ids\": tokenized_inputs[\"input_ids\"],\n",
        "            \"attention_mask\": tokenized_inputs[\"attention_mask\"],\n",
        "            \"labels\": tokenized_outputs[\"input_ids\"],\n",
        "        }\n",
        "\n",
        "        # Split data\n",
        "        total_examples = len(tokenized_data[\"input_ids\"])\n",
        "        train_size = int(0.9 * total_examples)\n",
        "\n",
        "        train_data = {k: v[:train_size] for k, v in tokenized_data.items()}\n",
        "        eval_data = {k: v[train_size:] for k, v in tokenized_data.items()}\n",
        "\n",
        "        train_dataset = Dataset.from_dict(train_data)\n",
        "        eval_dataset = Dataset.from_dict(eval_data)\n",
        "\n",
        "        return train_dataset, eval_dataset\n",
        "\n",
        "    def train(self, train_dataset: Dataset, eval_dataset: Dataset):\n",
        "        \"\"\"Train the model\"\"\"\n",
        "        print(\"Starting training...\")\n",
        "\n",
        "        # Disable W&B integration\n",
        "        os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=self.output_dir,\n",
        "            num_train_epochs=3,\n",
        "            per_device_train_batch_size=8,\n",
        "            per_device_eval_batch_size=8,\n",
        "            warmup_steps=100,\n",
        "            weight_decay=0.01,\n",
        "            logging_dir=f'{self.output_dir}/logs',\n",
        "            logging_steps=10,\n",
        "            evaluation_strategy=\"steps\",\n",
        "            eval_steps=100,\n",
        "            save_strategy=\"steps\",\n",
        "            save_steps=100,\n",
        "            load_best_model_at_end=True,\n",
        "            save_total_limit=2,\n",
        "            fp16=torch.cuda.is_available(),\n",
        "            remove_unused_columns=False,\n",
        "        )\n",
        "\n",
        "        data_collator = DataCollatorForSeq2Seq(\n",
        "            tokenizer=self.tokenizer,\n",
        "            model=self.model,\n",
        "            padding=True,\n",
        "            max_length=512\n",
        "        )\n",
        "\n",
        "        trainer = Trainer(\n",
        "            model=self.model,\n",
        "            args=training_args,\n",
        "            train_dataset=train_dataset,\n",
        "            eval_dataset=eval_dataset,\n",
        "            data_collator=data_collator,\n",
        "            tokenizer=self.tokenizer\n",
        "        )\n",
        "\n",
        "        print(\"Training model...\")\n",
        "        trainer.train()\n",
        "\n",
        "        print(\"Saving model...\")\n",
        "        trainer.save_model()\n",
        "        self.tokenizer.save_pretrained(self.output_dir)\n",
        "        print(f\"Model saved to {self.output_dir}\")\n",
        "\n",
        "def main():\n",
        "    print(\"Starting MIR FLAN-T5 QA Format Training Pipeline...\")\n",
        "\n",
        "    try:\n",
        "        # Mount Google Drive\n",
        "        drive.mount('/content/drive')\n",
        "\n",
        "        # Define paths\n",
        "        base_dir = '/content/drive/MyDrive/TFM2'\n",
        "        training_data_path = f\"{base_dir}/meli-training-content/simple_qa/flan_t5_training.json\"\n",
        "        output_dir = f\"{base_dir}/models/mir_flan_t5_qa\"\n",
        "\n",
        "        # Initialize trainer\n",
        "        trainer = MIRFlanT5TrainerQAFormat(\n",
        "            base_model=\"google/flan-t5-base\",\n",
        "            output_dir=output_dir\n",
        "        )\n",
        "\n",
        "        # Load training data\n",
        "        print(\"\\nLoading training data...\")\n",
        "        with open(training_data_path, 'r', encoding='utf-8') as f:\n",
        "            questions = json.load(f)\n",
        "        print(f\"Loaded {len(questions)} questions\")\n",
        "\n",
        "        # Print sample question for debugging\n",
        "        if questions:\n",
        "            print(\"\\nSample question format:\")\n",
        "            print(json.dumps(questions[0], indent=2))\n",
        "\n",
        "        # Prepare and split training data\n",
        "        train_dataset, eval_dataset = trainer.prepare_training_data(questions)\n",
        "\n",
        "        print(f\"\\nSplit sizes:\")\n",
        "        print(f\"Training examples: {len(train_dataset)}\")\n",
        "        print(f\"Validation examples: {len(eval_dataset)}\")\n",
        "\n",
        "        # Train the model\n",
        "        trainer.train(train_dataset, eval_dataset)\n",
        "\n",
        "        print(\"\\nTraining complete! Model saved to:\", output_dir)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        main()\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "        raise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NSrl9VI-De4G",
        "outputId": "e42ba31b-7668-464e-b9be-037c15eebf09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: fsspec 2024.6.1\n",
            "Uninstalling fsspec-2024.6.1:\n",
            "  Successfully uninstalled fsspec-2024.6.1\n",
            "Found existing installation: gcsfs 2024.6.1\n",
            "Uninstalling gcsfs-2024.6.1:\n",
            "  Successfully uninstalled gcsfs-2024.6.1\n",
            "Starting MIR FLAN-T5 QA Format Training Pipeline...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Initializing MIR FLAN-T5 Trainer...\n",
            "Loading base model: google/flan-t5-base\n",
            "Using device: cuda\n",
            "\n",
            "Loading training data...\n",
            "Loaded 619 questions\n",
            "\n",
            "Sample question format:\n",
            "{\n",
            "  \"id\": \"MED2019-P105\",\n",
            "  \"input\": \"Medical Question: 000 UIL. Se aporta imagen de la ecograf\\u00eda transvaginal. Qu\\u00e9 indicar\\u00eda en ese momento:\",\n",
            "  \"output\": \"Tratamiento con misoprostol por v\\u00eda sist\\u00e9mica. - 2 029102 ANICIDEM ESF\",\n",
            "  \"context\": {\n",
            "    \"year\": \"2019\",\n",
            "    \"question_number\": \"105\"\n",
            "  }\n",
            "}\n",
            "Preparing training data...\n",
            "Processed 100 questions...\n",
            "Processed 200 questions...\n",
            "Processed 300 questions...\n",
            "Processed 400 questions...\n",
            "Processed 500 questions...\n",
            "Processed 600 questions...\n",
            "Created 619 training examples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Split sizes:\n",
            "Training examples: 557\n",
            "Validation examples: 62\n",
            "Starting training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-5c7200c75d82>:145: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2691: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='210' max='210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [210/210 02:32, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>nan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>nan</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2691: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2691: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving model...\n",
            "Model saved to /content/drive/MyDrive/TFM2/models/mir_flan_t5_qa\n",
            "\n",
            "Training complete! Model saved to: /content/drive/MyDrive/TFM2/models/mir_flan_t5_qa\n"
          ]
        }
      ]
    }
  ]
}